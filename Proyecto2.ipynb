{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, roc_auc_score\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading and preparate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7895, 5)"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTraining = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/dataTesting.zip', encoding='UTF-8', index_col=0)\n",
    "dataTraining.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create y training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 1 1 ... 0 0 0]\n",
      " [0 1 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "###preparate y\n",
    "dataTraining['genres'] = dataTraining['genres'].map(lambda x: eval(x))\n",
    "le = MultiLabelBinarizer()\n",
    "y_genres = le.fit_transform(dataTraining['genres'])\n",
    "print(y_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Linking Data Testing and Data Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year' 'title' 'plot']\n",
      "['year' 'title' 'plot']\n"
     ]
    }
   ],
   "source": [
    "#print(dataTesting.columns.values)\n",
    "#print(dataTesting.head())\n",
    "dataTraining=pd.DataFrame(dataTraining)\n",
    "dataTraining.drop(['genres','rating'], axis='columns', inplace=True)\n",
    "print(dataTraining.columns.values)\n",
    "print(dataTesting.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7895, 3383)\n",
      "11278\n",
      "(11278, 3)\n"
     ]
    }
   ],
   "source": [
    "#print(pd.DataFrame(dataTraining).head())  ### se observan los indices de data training\n",
    "#pd.concat([pd.DataFrame(dataTraining),pd.DataFrame(dataTesting)],axis=0) ### se observa que en concatenacion \n",
    "# los indices que aparecen de primero son los de data training, e.d., data testing queda abajo de data training\n",
    "\n",
    "print((pd.DataFrame(dataTraining).shape[0],pd.DataFrame(dataTesting).shape[0]))\n",
    "print(pd.DataFrame(dataTraining).shape[0]+pd.DataFrame(dataTesting).shape[0]) \n",
    "union=pd.concat([pd.DataFrame(dataTraining),pd.DataFrame(dataTesting)],axis=0).copy()\n",
    "print(union.shape)\n",
    "\n",
    "############ Para extraer data testing de union:\n",
    "\n",
    "# union.reset_index(inplace=False).iloc[11278-3:11279,]       ## index 11278 dont exists in union (0:11277)\n",
    "\n",
    "\n",
    "#union.iloc[7895:11278,]  este es el subconjunto para recuperar datatesting\n",
    "#dataTesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Linking plot and title from union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(union['title'].iloc[0:1,])\n",
    "#print(union['plot'].iloc[0:1,])\n",
    "#print((union['title']+\" \"+union['plot']).iloc[0,])\n",
    "union['text']=pd.DataFrame(union['title']+\" \"+union['plot'])  ##by row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Grammar  Characterization from union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas= pd.DataFrame(columns=['zNoclasif','zCC','zCD','zDT','zEX','zFW','zIN','zJJ','zJJR','zJJS','zLS','zMD','zNN','zNNS','zNNP','zNNPS','zPDT','zPOS','zPRP','zPRP$ ','zRB','zRBR','zRBS','zRP','zTO','zUH','zVB','zVBD','zVBG','zVBN','zVBP','zVBZ','zWDT','zWP','zWP$','zWRB'])\n",
    "\n",
    "union = pd.concat(list(union.align(columnas)),ignore_index=False)\n",
    "union=union.reset_index(drop=True)                                      ##!!!\n",
    "union = union.iloc[0:int(len(union)/2),:]\n",
    "union=union.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "union['text']=union['text'].map(lambda x: re.sub(r'\\W+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2500443341017911\n",
      "0.2501330023053733\n",
      "0.2502216705089555\n",
      "0.25031033871253766\n",
      "0.2503990069161199\n",
      "0.2504876751197021\n",
      "0.25057634332328427\n",
      "0.2506650115268665\n",
      "0.25075367973044865\n",
      "0.2508423479340309\n",
      "0.25093101613761304\n",
      "0.25101968434119526\n",
      "0.2511083525447774\n",
      "0.25119702074835965\n",
      "0.2512856889519418\n",
      "0.25137435715552403\n",
      "0.25146302535910625\n",
      "0.2515516935626884\n",
      "0.25164036176627064\n",
      "0.2517290299698528\n",
      "0.251817698173435\n",
      "0.2519063663770172\n",
      "0.2519950345805994\n",
      "0.2520837027841816\n",
      "0.2521723709877638\n",
      "0.25226103919134596\n",
      "0.2523497073949282\n",
      "0.2524383755985104\n",
      "0.25252704380209257\n",
      "0.2526157120056748\n",
      "0.25270438020925695\n",
      "0.25279304841283917\n",
      "0.25288171661642134\n",
      "0.25297038482000356\n",
      "0.2530590530235857\n",
      "0.25314772122716794\n",
      "0.2532363894307501\n",
      "0.25332505763433233\n",
      "0.25341372583791455\n",
      "0.2535023940414967\n",
      "0.25359106224507894\n",
      "0.2536797304486611\n",
      "0.2537683986522433\n",
      "0.2538570668558255\n",
      "0.2539457350594077\n",
      "0.25403440326298987\n",
      "0.2541230714665721\n",
      "0.25421173967015426\n",
      "0.2543004078737365\n",
      "0.2543890760773187\n",
      "0.25447774428090086\n",
      "0.2545664124844831\n",
      "0.25465508068806525\n",
      "0.25474374889164747\n",
      "0.25483241709522964\n",
      "0.25492108529881186\n"
     ]
    }
   ],
   "source": [
    "for h in range(0,len(union)):\n",
    "    letras=union['text'].iloc[h:h+1].tolist()\n",
    "    tokens=nltk.word_tokenize(str(letras))\n",
    "    t_letre=nltk.pos_tag(tokens)\n",
    "    tablita = pd.DataFrame(t_letre,columns=['Palabra','Tipo'])\n",
    "    tabla = pd.pivot_table(tablita, values='Palabra', index=['Tipo'], aggfunc='count')\n",
    "    tabla = pd.DataFrame(data=tabla)\n",
    "    resultado = tabla['Palabra'].astype(str).str.split(\" \", n = 1, expand = True) \n",
    "    prueba = resultado.values.tolist()\n",
    "    df1 = pd.DataFrame(tabla.iloc[:,1:])\n",
    "    df1 = df1.reset_index()\n",
    "    df2 = pd.DataFrame(prueba)\n",
    "    df3 = pd.DataFrame(data=df1,columns=['Tipo','Cantidad'])\n",
    "    df3['Cantidad']=df2\n",
    "    gramar = ['CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS','NNP','NNPS','PDT','POS','PRP','PRP$ ','RB','RBR','RBS','RP','TO','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB']\n",
    "    a=6\n",
    "    if (h/len(union)>=0.25 and h/len(union)<=0.255):\n",
    "        print(h/len(union))\n",
    "        \n",
    "    if (h/len(union)>=0.5 and h/len(union)<=0.55):\n",
    "        print(h/len(union))\n",
    "        \n",
    "    if (h/len(union)>=0.9 and h/len(union)<=0.95):\n",
    "        print(h/len(union))\n",
    "        \n",
    "    #print(h)\n",
    "    #print(len(dataTraining))\n",
    "    for i in gramar:\n",
    "        for j in range(0,len(df3)):\n",
    "            if df3['Tipo'].loc[j] == i: \n",
    "                union.iloc[h:h+1,a:a+1] = int(df3['Cantidad'].loc[j])\n",
    "        a=a+1 \n",
    "union.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bmhm2qMRG5_Y"
   },
   "source": [
    "## 1.5 Tokenization text feature from union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating consistency data set union:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "WH9n1SCnG6ms",
    "outputId": "9f6a12eb-3992-400c-d8ee-2823a3dd8f48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11278, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>zCC</th>\n",
       "      <th>zCD</th>\n",
       "      <th>zDT</th>\n",
       "      <th>zEX</th>\n",
       "      <th>zFW</th>\n",
       "      <th>zIN</th>\n",
       "      <th>zJJ</th>\n",
       "      <th>zJJR</th>\n",
       "      <th>zJJS</th>\n",
       "      <th>...</th>\n",
       "      <th>zVB</th>\n",
       "      <th>zVBD</th>\n",
       "      <th>zVBG</th>\n",
       "      <th>zVBN</th>\n",
       "      <th>zVBP</th>\n",
       "      <th>zVBZ</th>\n",
       "      <th>zWDT</th>\n",
       "      <th>zWP</th>\n",
       "      <th>zWP$</th>\n",
       "      <th>zWRB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11278.000000</td>\n",
       "      <td>11278.0</td>\n",
       "      <td>11278.0</td>\n",
       "      <td>11278.000000</td>\n",
       "      <td>11278.000000</td>\n",
       "      <td>11278.000000</td>\n",
       "      <td>11278.000000</td>\n",
       "      <td>11278.000000</td>\n",
       "      <td>11278.000000</td>\n",
       "      <td>11278.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11278.000000</td>\n",
       "      <td>11278.000000</td>\n",
       "      <td>11278.000000</td>\n",
       "      <td>11278.000000</td>\n",
       "      <td>11278.000000</td>\n",
       "      <td>11278.000000</td>\n",
       "      <td>11278.000000</td>\n",
       "      <td>11278.000000</td>\n",
       "      <td>11278.000000</td>\n",
       "      <td>11278.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1989.555861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.955489</td>\n",
       "      <td>1.128835</td>\n",
       "      <td>13.093988</td>\n",
       "      <td>0.104096</td>\n",
       "      <td>0.082018</td>\n",
       "      <td>14.961607</td>\n",
       "      <td>11.485636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>5.223089</td>\n",
       "      <td>1.823373</td>\n",
       "      <td>2.851835</td>\n",
       "      <td>2.912041</td>\n",
       "      <td>2.689927</td>\n",
       "      <td>7.352101</td>\n",
       "      <td>0.579447</td>\n",
       "      <td>1.210853</td>\n",
       "      <td>0.061979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>22.700002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.861664</td>\n",
       "      <td>1.330597</td>\n",
       "      <td>8.964654</td>\n",
       "      <td>0.352821</td>\n",
       "      <td>0.522537</td>\n",
       "      <td>10.842739</td>\n",
       "      <td>7.880409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031222</td>\n",
       "      <td>4.411352</td>\n",
       "      <td>2.405990</td>\n",
       "      <td>2.804647</td>\n",
       "      <td>2.745909</td>\n",
       "      <td>2.730622</td>\n",
       "      <td>6.004704</td>\n",
       "      <td>0.984689</td>\n",
       "      <td>1.641134</td>\n",
       "      <td>0.264623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1893.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1979.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1997.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2007.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2015.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>209.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               year      zCC      zCD           zDT           zEX  \\\n",
       "count  11278.000000  11278.0  11278.0  11278.000000  11278.000000   \n",
       "mean    1989.555861      0.0      0.0      4.955489      1.128835   \n",
       "std       22.700002      0.0      0.0      3.861664      1.330597   \n",
       "min     1893.000000      0.0      0.0      0.000000      0.000000   \n",
       "25%     1979.000000      0.0      0.0      2.000000      0.000000   \n",
       "50%     1997.000000      0.0      0.0      4.000000      1.000000   \n",
       "75%     2007.000000      0.0      0.0      7.000000      2.000000   \n",
       "max     2015.000000      0.0      0.0     84.000000     16.000000   \n",
       "\n",
       "                zFW           zIN           zJJ          zJJR          zJJS  \\\n",
       "count  11278.000000  11278.000000  11278.000000  11278.000000  11278.000000   \n",
       "mean      13.093988      0.104096      0.082018     14.961607     11.485636   \n",
       "std        8.964654      0.352821      0.522537     10.842739      7.880409   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        7.000000      0.000000      0.000000      8.000000      6.000000   \n",
       "50%       11.000000      0.000000      0.000000     12.000000     10.000000   \n",
       "75%       17.000000      0.000000      0.000000     19.000000     15.000000   \n",
       "max      222.000000      5.000000     27.000000    209.000000    110.000000   \n",
       "\n",
       "           ...                zVB          zVBD          zVBG          zVBN  \\\n",
       "count      ...       11278.000000  11278.000000  11278.000000  11278.000000   \n",
       "mean       ...           0.000798      5.223089      1.823373      2.851835   \n",
       "std        ...           0.031222      4.411352      2.405990      2.804647   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.000000      2.000000      0.000000      1.000000   \n",
       "50%        ...           0.000000      4.000000      1.000000      2.000000   \n",
       "75%        ...           0.000000      7.000000      2.000000      4.000000   \n",
       "max        ...           2.000000     84.000000     33.000000     42.000000   \n",
       "\n",
       "               zVBP          zVBZ          zWDT           zWP          zWP$  \\\n",
       "count  11278.000000  11278.000000  11278.000000  11278.000000  11278.000000   \n",
       "mean       2.912041      2.689927      7.352101      0.579447      1.210853   \n",
       "std        2.745909      2.730622      6.004704      0.984689      1.641134   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        1.000000      1.000000      3.000000      0.000000      0.000000   \n",
       "50%        2.000000      2.000000      6.000000      0.000000      1.000000   \n",
       "75%        4.000000      4.000000     10.000000      1.000000      2.000000   \n",
       "max       40.000000     50.000000    124.000000     10.000000     15.000000   \n",
       "\n",
       "               zWRB  \n",
       "count  11278.000000  \n",
       "mean       0.061979  \n",
       "std        0.264623  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        3.000000  \n",
       "\n",
       "[8 rows x 37 columns]"
      ]
     },
     "execution_count": 793,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#union.fillna(0, inplace=True)\n",
    "print(union.shape)\n",
    "union.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0fXAdPl6NHwY"
   },
   "source": [
    "\n",
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZqW4K7rcJNx9",
    "outputId": "bc84ec35-2a72-4b14-8bc9-697442e8de7f"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "vect = CountVectorizer(ngram_range=(0,3),max_features=2000)\n",
    "input_text = vect.fit_transform(union['text'])\n",
    "\n",
    "#############\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "input_text = tfidf_transformer.fit_transform(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZqW4K7rcJNx9",
    "outputId": "bc84ec35-2a72-4b14-8bc9-697442e8de7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11278, 2000)\n",
      "(11278, 40)\n",
      "(11278, 2040)\n"
     ]
    }
   ],
   "source": [
    "###Verificar si funciona3\n",
    "print(input_text.shape)\n",
    "print(union.shape)\n",
    "union=pd.concat([union,pd.DataFrame(input_text.todense())],axis=1).copy()\n",
    "print(pd.DataFrame(union).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iy0TRKTIK5wC"
   },
   "source": [
    "## 1.6 Year feature tratated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "union['year1'] = pd.cut(union.year,\n",
    "                bins=[1890,1900,1910,1920,1930,1940,1950,1960,1970,1980,1990,2000,2010,2020],\n",
    "                labels=[\"1890\",\"1900\",\"1910\",\"1920\",\"1930\",\"1940\",\"1950\",\"1960\",\"1970\",\"1980\",\"1990\",\"2000\",\"2010\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "OSYyyAhdQh0F",
    "outputId": "e5fcd42a-60d0-4716-84dd-668df3809018"
   },
   "outputs": [],
   "source": [
    "input_year=pd.get_dummies(union['year1'])\n",
    "input_year.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1890</th>\n",
       "      <th>1900</th>\n",
       "      <th>1910</th>\n",
       "      <th>1920</th>\n",
       "      <th>1930</th>\n",
       "      <th>1940</th>\n",
       "      <th>1950</th>\n",
       "      <th>1960</th>\n",
       "      <th>1970</th>\n",
       "      <th>1980</th>\n",
       "      <th>1990</th>\n",
       "      <th>2000</th>\n",
       "      <th>2010</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1890  1900  1910  1920  1930  1940  1950  1960  1970  1980  1990  2000  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0     0     1   \n",
       "1     0     0     0     0     0     0     0     0     0     0     0     1   \n",
       "2     0     0     0     0     0     1     0     0     0     0     0     0   \n",
       "3     0     0     0     0     0     0     1     0     0     0     0     0   \n",
       "4     0     0     0     0     0     0     0     0     0     1     0     0   \n",
       "\n",
       "   2010  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  "
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_year.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eykenuL5VSyx"
   },
   "source": [
    "# 2. Consolidation input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union.drop('year1', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mvz7gLZ7VPyz",
    "outputId": "0fcf4c34-7754-470d-b142-f3818ad87edb",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11278, 13)"
      ]
     },
     "execution_count": 800,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1=pd.concat([union,input_year],axis=1).copy()\n",
    "input1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPeUza31fPoq"
   },
   "source": [
    "# 3. Train_Test_Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input1.head()\n",
    "datatr=input1.iloc[0:7895,4:input1.shape[1]]\n",
    "datatest=input1.iloc[7895:11278,4:input1.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_zqqrVbfWFHu"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_genres, y_test_genres = train_test_split(datatr, y_genres, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.astype(float)\n",
    "X_test=X_test.astype(float)\n",
    "y_train_genres=y_train_genres.astype(float)\n",
    "y_test_genres=y_test_genres.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc = StandardScaler()\n",
    "#sc.fit_transform(X_train)\n",
    "#X_test = sc.transform(X_test)\n",
    "#datatest=sc.transform(datatest).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ACBBzoVst5H"
   },
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gEGnEoVUFYg9"
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as pp\n",
    "xx=pp.csr_matrix(X_train)\n",
    "yy=pp.csr_matrix(y_train_genres)\n",
    "xt=pp.csr_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "clf1 = OneVsRestClassifier(XGBClassifier(n_estimators=300,max_depth=10,n_jobs=-1, random_state=42,learning_rate=0.02))\n",
    "clf1.fit(xx,yy)\n",
    "#y_prob1 = clf1.predict(xt)\n",
    "y_prob_m1 = clf1.predict_proba(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Q7bjcLax61MM",
    "outputId": "f227cf3a-fa91-4fca-961a-72d03e97f4f1"
   },
   "outputs": [],
   "source": [
    "clf2 = OneVsRestClassifier(RandomForestClassifier(n_jobs=-1, n_estimators=1500, max_depth=10, random_state=42))\n",
    "clf2.fit(xx,yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "_zUw1C_VkSsr",
    "outputId": "8ff3f4af-f025-4252-9685-1d5959b7e232"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf3 = OneVsRestClassifier(GradientBoostingClassifier(n_estimators=500),n_jobs=-1)\n",
    "clf3.fit(xx,yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "dvJ5SYn_lQg5",
    "outputId": "b3002369-2ad3-4807-e985-53e16634e9f9"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf4 = OneVsRestClassifier(AdaBoostClassifier(n_estimators=200),n_jobs=-1)\n",
    "clf4.fit(xx,yy) #####\n",
    "y_prob_m4 = clf4.predict_proba(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "Vkg53Cxid0Uu",
    "outputId": "5a961837-0b7f-4959-fb3e-bcbc8201bb0c"
   },
   "outputs": [],
   "source": [
    "#from sklearn.svm import SVC # \"Support Vector Classifier\"\n",
    "#clf = OneVsRestClassifier(SVC(kernel='rbf',probability=True))\n",
    "#clf.fit(xx,yy) \n",
    "#y_prob_m2 = clf.predict_proba(xt)\n",
    "#del clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_m1 = clf2.predict_proba(xt)\n",
    "y_prob_m2 = clf3.predict_proba(xt)\n",
    "y_prob_m3 = clf2.predict_proba(xt)\n",
    "y_prob_m4 = clf3.predict_proba(xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2YAkAF4el39i"
   },
   "source": [
    "## 4.1 ROC'S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M78WHDR2-4YI",
    "outputId": "949454df-1c4e-40ba-931c-19679b58fa2c"
   },
   "outputs": [],
   "source": [
    "y_tot=(y_prob_m1 + y_prob_m2 + y_prob_m3 + y_prob_m4)/4\n",
    "roc_auc_score(y_test_genres, y_tot, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 KAGGLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dLiGD9UsAOLB"
   },
   "source": [
    "## 5.1 Predict the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_m1 = clf2.predict_proba(xt)\n",
    "y_prob_m2 = clf3.predict_proba(xt)\n",
    "y_prob_m3 = clf2.predict_proba(xt)\n",
    "y_prob_m4 = clf3.predict_proba(xt)\n",
    "ytot=(y_prob_m1+y_prob_m2+y_prob_m3+y_prob_m4)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
    "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
    "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
    "\n",
    "#y_pred_test_genres = clf.predict_proba(datatest.iloc[,4:input1.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(y_tot, index=dataTesting.index, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('pred_genres_text_RF3.csv', index_label='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API - Rest Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias usadas\n",
    "import nltk\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from flask import Flask\n",
    "from flask_restful import Api, Resource, reqparse\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.sparse as pp\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "api = Api(app)\n",
    "\n",
    "dataTraining = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)\n",
    "dataTraining['genres'] = dataTraining['genres'].map(lambda x: eval(x))\n",
    "\n",
    "le = MultiLabelBinarizer()\n",
    "y_genres = le.fit_transform(dataTraining['genres'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas= pd.DataFrame(columns=['zNoclasif','zCC','zCD','zDT','zEX','zFW','zIN','zJJ','zJJR','zJJS','zLS','zMD','zNN','zNNS','zNNP','zNNPS','zPDT','zPOS','zPRP','zPRP$ ','zRB','zRBR','zRBS','zRP','zTO','zUH','zVB','zVBD','zVBG','zVBN','zVBP','zVBZ','zWDT','zWP','zWP$','zWRB'])\n",
    "gramar = ['CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS','NNP','NNPS','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','TO','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB']\n",
    "vect = CountVectorizer(max_features=2000,ngram_range=(1,50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prediccion(Resource):\n",
    "        def get(self, titulo, plot,ano):\n",
    "                #Liberar memoria por cada petición\n",
    "                gc.collect()\n",
    "                #Carga de modelos\n",
    "                clf1 = joblib.load(os.path.dirname(__file__) + 'Modelo1.pkl')\n",
    "                clf2 = joblib.load(os.path.dirname(__file__) + 'Modelo1.pkl')\n",
    "                clf3 = joblib.load(os.path.dirname(__file__) + 'Modelo1.pkl')\n",
    "                clf4 = joblib.load(os.path.dirname(__file__) + 'Modelo4.pkl')\n",
    "                # Tratamiento de titulo, sinapsis y año\n",
    "                text = titulo+\" \"+plot\n",
    "                union=pd.DataFrame(data=[int(ano)],columns=['year'])\n",
    "                union['year'] = pd.cut(union.year,bins=[1890,1900,1910,1920,1930,1940,1950,1960,1970,1980,1990,2000,2010,2020],labels=[\"1890\",\"1900\",\"1910\",\"1920\",\"1930\",\"1940\",\"1950\",\"1960\",\"1970\",\"1980\",\"1990\",\"2000\",\"2010\"])\n",
    "                input_year_t=pd.get_dummies(union['year'])\n",
    "                input_year_t.reset_index(inplace=True,drop=True)\n",
    "                input_t = pd.DataFrame([text],columns=['text'])\n",
    "                input_t['text'] = input_t['text'].map(lambda x: re.sub(r'\\W+', ' ', x))\n",
    "                input_t = pd.concat(list(input_t.align(columnas)),ignore_index=False)\n",
    "                input_t = input_t.reset_index(drop=True)\n",
    "                input_t = input_t.iloc[0:int(len(input_t)/2),:]\n",
    "                input_t = input_t.reset_index(drop=True)\n",
    "                palabras = input_t['text'].iloc[0:1].tolist()\n",
    "                tokens=nltk.word_tokenize(str(palabras))\n",
    "                t_letre=nltk.pos_tag(tokens)\n",
    "                tb1 = pd.DataFrame(t_letre,columns=['Palabra','Tipo'])\n",
    "                tb2 = pd.pivot_table(tb1, values='Palabra', index=['Tipo'], aggfunc='count')\n",
    "                rta = tb2['Palabra'].astype(str).str.split(\" \", n = 1, expand = True)\n",
    "                pb = rta.values.tolist()\n",
    "                df1 = pd.DataFrame(tb1.iloc[:,1:])\n",
    "                df2 = pd.DataFrame(pb)\n",
    "                df3 = pd.DataFrame(df1,columns=['Tipo','Cantidad'])\n",
    "                df3['Cantidad']=df2\n",
    "                df3['Cantidad'].fillna(0, inplace=True)\n",
    "                a=1\n",
    "                for i in gramar:\n",
    "                        for j in range(0,len(df3)):\n",
    "                                if df3['Tipo'].loc[j] == i:\n",
    "                                        input_t.iloc[0:1,a:a+1] = int(df3['Cantidad'].loc[j])\n",
    "                        a=a+1\n",
    "                input_t.fillna(0, inplace=True)\n",
    "                input_text_t = vect.fit_transform(input_t['text'])\n",
    "                input_gramar_t = input_t.iloc[:,1:37]\n",
    "                input_t_T=pd.concat([pd.DataFrame(input_gramar_t),pd.DataFrame(input_text_t.todense()),pd.DataFrame(data=[int(ano)]),input_year_t],axis=1,ignore_index=False)\n",
    "                input_t_T = input_t_T.iloc[0:1,:]\n",
    "                xt_r = pp.csr_matrix(input_t_T)\n",
    "                   \n",
    "                #Predicción    \n",
    "                y_pred_test_genres_a = clf1.predict_proba(xt_r)\n",
    "                y_pred_test_genres_b = clf2.predict_proba(xt_r)\n",
    "                y_pred_test_genres_c = clf3.predict_proba(xt_r)\n",
    "                y_pred_test_genres_d = clf4.predict_proba(xt_r)\n",
    "                y_pred_test_genres_t = (y_pred_test_genres_a+y_pred_test_genres_d+y_pred_test_genres_c+y_pred_test_genres_d)/4\n",
    "                y_pred_test_genres_t.astype(int)\n",
    "                y_pred_test_genres_t = pd.DataFrame(y_pred_test_genres_t)\n",
    "                a=y_pred_test_genres_t.replace({0: False, 1: True})\n",
    "                a=le.classes_[a.values.tolist()]\n",
    "                a = str(a)\n",
    "                return a\n",
    "\n",
    "\n",
    "api.add_resource(prediccion,\"/variables/<string:titulo>/<string:plot>/<string:ano>\")\n",
    "\n",
    "app.run(debug=True, use_reloader=False, host='0.0.0.0', port=6800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://ec2-18-191-195-115.us-east-2.compute.amazonaws.com:6800/variables/Most/most is the story of a single father who takes his eight year - old son to work with him at the railroad drawbridge where he is the bridge tender .  a day before ,  the boy meets a woman boarding a train ,  a drug abuser .  at the bridge ,  the father goes into the engine room ,  and tells his son to stay at the edge of the nearby lake .  a ship comes ,  and the bridge is lifted .  though it is supposed to arrive an hour later ,  the train happens to arrive .  the son sees this ,  and tries to warn his father ,  who is not able to see this .  just as the oncoming train approaches ,  his son falls into the drawbridge gear works while attempting to lower the bridge ,  leaving the father with a horrific choice .  the father then lowers the bridge ,  the gears crushing the boy .  the people in the train are completely oblivious to the fact a boy died trying to save them ,  other than the drug addict woman ,  who happened to look out her train window .  the movie ends ,  with the man wandering a new city ,  and meets the woman ,  no longer a drug addict ,  holding a small baby .  other relevant narratives run in parallel ,  namely one of the female drug - addict ,  and they all meet at the climax of this tumultuous film/2003\n",
    "\n",
    "http://ec2-18-191-195-115.us-east-2.compute.amazonaws.com:6800/variables/Pursuit to Algiers/holmes and watson are recruited in a serpentine fashion to escort the heir to a european throne back to his native country following his father ' s assassination .  because the prince has been educated in great britain ,  holmes persuades him to masquerade as watson ' s nephew nicolas on a ocean liner bound for algiers .  unfortunately the ship is filled with red herrings as well as real assassins ,  and holmes is challenged to outwit them all and deliver his charge to his destination .  among the suspects are a knife - throwing circus performer ,  two shadowy archaeologists ,  a hulking deaf mute ,  an enigmatic ship ' s steward ,  a chanteuse with a mysterious song case ,  and a gun - toting british dowager/1945\n",
    "\n",
    "http://ec2-18-191-195-115.us-east-2.compute.amazonaws.com:6800/variables/Aftershock/in santiago ,  the american gringo is spending vacation with his chilean friends ariel and the wealthy and influent pollo .  they meet the russian tourists irina ,  the promiscuous kylie and her protective sister monica and pollo invites them to go with his friends and him to valparaiso .  during the night ,  they go to a nightclub but an earthquake collapses the club and there is a warning of tsunami .  the group tries to find a safe place in the middle of the chaos .  meanwhile a nearby prison also collapses and dangerous criminals escape .  when they see the women on the streets ,  they hunt them down along the nightmarish night /2012"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
